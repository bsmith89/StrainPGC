# Imports
import textwrap
import pandas as pd
import os.path


snakemake.utils.min_version("8.20")


include: "rules/common.smk"


# Config  # Should go in config.yaml


## Workflow config.


# This is all of the default config values, and does not include any data-specific
# configuration.
configfile: "workflow/config.yaml"


container: config["main_container"]


rule all:
    input:
        targets=[prefix_workdir(f) for f in config["target_all_noprefix"]],


rule start_shell:
    shell:
        "bash"


rule start_shell_gtpro:
    container:
        config["gtpro_container"]
    shell:
        "bash"


# Metadata  # Paths to metadata files should go in project specific config.yaml?


## Load metagenome paths
config["samples"] = pd.read_table(config["sample_manifest_path"], index_col="mgen")
config["all_samples"] = config["samples"].index


# Pre-processing Metagenomes
## Link input reads
rule symlink_input_reads:
    output:
        r1=prefix_workdir("reads/{mgen}/preprocess/r1.fq.gz"),
        r2=prefix_workdir("reads/{mgen}/preprocess/r2.fq.gz"),
    input:
        r1=lambda w: config["samples"].loc[w.mgen]["r1_path"],
        r2=lambda w: config["samples"].loc[w.mgen]["r2_path"],
    shell:
        """
        ln -rs {input.r1} {output.r1}
        ln -rs {input.r2} {output.r2}
        """


## Deduplicate
rule deduplicate_reads:
    output:
        r1=temp(prefix_workdir("reads/{mgen}/preprocess/r1{stem}dedup.fq.gz")),
        r2=temp(prefix_workdir("reads/{mgen}/preprocess/r2{stem}dedup.fq.gz")),
    input:
        script="workflow/scripts/fastuniq_wrapper.sh",
        r1=prefix_workdir("reads/{mgen}/preprocess/r1{stem}fq.gz"),
        r2=prefix_workdir("reads/{mgen}/preprocess/r2{stem}fq.gz"),
    resources:
        mem_mb=10_000,
    shell:
        "{input.script} {input.r1} {input.r2} {output.r1} {output.r2}"


## Adapter trim
rule trim_adapters:
    output:
        fq=temp(prefix_workdir("reads/{mgen}/preprocess/{stem}.deadapt.fq.gz")),
    input:
        adapters=config["illumina_adapters_fasta_path"],
        fq=prefix_workdir("reads/{mgen}/preprocess/{stem}.fq.gz"),
    log:
        prefix_workdir("reads/{mgen}/preprocess/{stem}.deadapt.log"),
    threads: 2
    shell:
        """
        scythe -a {input.adapters} {input.fq} 2>{log} | gzip -c > {output.fq}
        ! grep -Fxq 'Blank FASTA header or sequence in adapters file.' {log}
        """


## Quality trim
rule quality_trim_reads:
    output:
        r1=temp(prefix_workdir("reads/{mgen}/preprocess/r1{stem}qtrim.fq.gz")),
        r2=temp(prefix_workdir("reads/{mgen}/preprocess/r2{stem}qtrim.fq.gz")),
        r3=temp(prefix_workdir("reads/{mgen}/preprocess/r3{stem}qtrim.fq.gz")),
    input:
        r1=prefix_workdir("reads/{mgen}/preprocess/r1{stem}fq.gz"),
        r2=prefix_workdir("reads/{mgen}/preprocess/r2{stem}fq.gz"),
    params:
        qual_type="sanger",
        qual_thresh=20,
    shell:
        """
        sickle pe -t {params.qual_type} -q {params.qual_thresh} --gzip-output \
            --pe-file1 {input.r1} --pe-file2 {input.r2} \
            --output-pe1 {output.r1} --output-pe2 {output.r2} \
            --output-single {output.r3}
        """


## Alias Preprocessed Reads
rule alias_preprocessed_reads:
    output:
        prefix_workdir("reads/{mgen}/preprocess/{stem}.proc.fq.gz"),
    input:
        prefix_workdir(
            f"reads/{{mgen}}/preprocess/{{stem}}.{config['preprocessing_infix']}.fq.gz"
        ),
    shell:
        "ln {input} {output}"


# MIDAS and Pangenome Profiling
## Initialize MIDASDB locally
rule initialize_midasdb_for_species_list:
    output:
        flag=prefix_workdir("midasdb_initialized.flag"),
    input:
        species_list=config["species_list_path"],
    params:
        midasdb_dir=config["midasdb_dir"],
    threads: 8
    shell:
        """
        midas database --species_list {input.species_list} --num_cores {threads} --download --midasdb_dir {params.midasdb_dir}
        midas prune_centroids --species $(tr '\\n' ',' < {input.species_list} | sed 's:,$::') --midasdb_dir {params.midasdb_dir} --midasdb_name localdb -t {threads} --prune_method median --prune_cutoff 0.4 --remove_singleton --debug --force
        touch {output}
        """


rule symlink_midasdb_gene_metadata:
    output:
        prefix_workdir("species/{species}/midas/clusters_99_info.tsv"),
    input:
        prefix_workdir("midasdb_initialized.flag"),
    params:
        dir=config["midasdb_dir"],
    shell:
        """
        ln -rs $(realpath "{params.dir}/pangenomes/{wildcards.species}/clusters_99_info.tsv") {output}
        """


## Build MIDASDB index
rule build_midas_pangenomes_bowtie_index:
    output:
        dir=directory(prefix_workdir("pangenomes.bt2.d")),
    input:
        midasdb_flag=prefix_workdir("midasdb_initialized.flag"),
        species_list=config["species_list_path"],
    params:
        midasdb_name="uhgg",
        midasdb_dir=config["midasdb_dir"],
    threads: 60
    resources:
        mem_mb=100_000,
    shell:
        """
        midas build_bowtie2db \
                --debug \
                --bt2_indexes_dir {output.dir} \
                --bt2_indexes_name pangenomes \
                --midasdb_name {params.midasdb_name} \
                --midasdb_dir {params.midasdb_dir} \
                --select_threshold=-1 \
                --species_list {input.species_list} \
                --num_cores {threads}
                # --prune_centroids \
                # --remove_singleton
        """


## Align
rule run_midas_genes_align_only:
    output:
        bam=(prefix_workdir("reads/{mgen}/midas/pangenome_profile_raw.bam")),
    input:
        species_list=config["species_list_path"],
        bowtie_indexes=prefix_workdir("pangenomes.bt2.d"),
        r1=prefix_workdir("reads/{mgen}/preprocess/r1.proc.fq.gz"),
        r2=prefix_workdir("reads/{mgen}/preprocess/r2.proc.fq.gz"),
    params:
        outdir=prefix_workdir("reads/{mgen}/midas/pangenome_profile.midas.d"),
        outbam=prefix_workdir(
            "reads/{mgen}/midas/pangenome_profile.midas.d/{mgen}/genes/{mgen}.bam"
        ),
        midasdb_dir=config["midasdb_dir"],
        midasdb_name="uhgg",
    threads: 4
    resources:
        mem_mb=30_000,
    shell:
        """
        midas run_genes --num_cores {threads} \
                -1 {input.r1} -2 {input.r2} \
                --sample_name {wildcards.mgen} \
                --midasdb_name {params.midasdb_name} \
                --prebuilt_bowtie2_indexes {input.bowtie_indexes}/pangenomes \
                --prebuilt_bowtie2_species {input.species_list} \
                --select_threshold=-1 \
                --aln_speed sensitive \
                --aln_extra_flags '--mm --ignore-quals' \
                --total_depth 0 \
                --cluster_level 99 \
                --alignment_only \
                {params.outdir}
        ln {params.outbam} {output.bam}
        """


## Profile
rule midas_profile_genes:
    output:
        temp(
            prefix_workdir(
                "reads/{mgen}/midas/pangenome_profile_raw.gene_mapping_tally.tsv.bz2"
            )
        ),
    input:
        prefix_workdir("reads/{mgen}/midas/pangenome_profile_raw.bam"),
    threads: 1
    shell:
        """
        samtools depth -@ {threads} -g SECONDARY --min-MQ 0 {input} \
            | awk -v OFS='\t' '\\
                BEGIN {{gene_id="__START__"; depth_tally=0}} \\
                $1==gene_id {{depth_tally+=$3}} \\
                $1!=gene_id {{print gene_id, depth_tally; gene_id=$1; depth_tally=0}} \\
                END {{print gene_id,depth_tally}} \\
                ' \
            | (echo 'gene_id\ttally'; sed '1,1d') \
            | bzip2 -zc > {output}.temp
        mv {output}.temp {output}
        """


## Merge
rule merge_and_aggregate_pangenome_profiles:
    output:
        temp(prefix_workdir("species/{species}/midas/pangenome_profile.depth.tsv.bz2")),
    input:
        script="workflow/scripts/merge_and_aggregate_pangenome_profiles.py",  # FIXME: Write this script
        samples=expand(
            prefix_workdir(
                "reads/{mgen}/midas/pangenome_profile_raw.gene_mapping_tally.tsv.bz2"
            ),
            mgen=config["all_samples"],
        ),
        gene_info=prefix_workdir("species/{species}/midas/clusters_99_info.tsv"),
    params:
        args=expand(
            "{mgen}="
            + prefix_workdir(
                "reads/{mgen}/midas/pangenome_profile_raw.gene_mapping_tally.tsv.bz2"
            ),
            mgen=config["all_samples"],
        ),
        agg_col="centroid_" + config["pangenome_profiling_agg_level"],
    threads: 1
    resources:
        mem_mb=20_000,
    shell:
        """
        {input.script} {input.gene_info} {params.agg_col} {output} {params.args}
        """


# GT-Pro
## Run core algorithm
rule run_gtpro:
    output:
        temp(prefix_workdir("reads/{mgen}/gtpro/{r}.snv_counts_raw.gz")),
    input:
        r=prefix_workdir("reads/{mgen}/preprocess/{r}.proc.fq.gz"),
        db=config["gtpro_ref_dir"],
    params:
        db_l=32,
        db_m=36,
        db_name=config["gtpro_ref_name"],
    threads: 8
    container:
        config["gtpro_container"]  # NOTE (2024-09-26): There seems to be a unresolved bug with GT-Pro containers built using g++ v9.4.0.  See https://github.com/zjshi/gt-pro/issues/48
    resources:
        mem_mb=10_000,
    shell:
        """
        GT_Pro genotype -t {threads} -l {params.db_l} -m {params.db_m} -d {params.db_name} -o {output}.temp {input.r}
        mv {output}.temp.tsv.gz {output}
        """


## Postprocess
rule gtpro_finish_processing_reads:
    output:
        temp(prefix_workdir("reads/{mgen}/gtpro/{r}.snv_counts.tsv.bz2")),
    input:
        raw=prefix_workdir("reads/{mgen}/gtpro/{r}.snv_counts_raw.gz"),
        db=os.path.join(
            config["gtpro_ref_dir"], "variants_main.covered.hq.snp_dict.tsv"
        ),
    # container:  # NOTE (2024-09-26): Does not require the GT-Pro container because it's just a python script.
    #     config['gtpro_container']
    shell:
        "GT_Pro parse --dict {input.db} --in <(zcat {input.raw}) --out >(bzip2 -c > {output})"


## Merge
rule merge_gtpro_counts_for_species:
    output:
        temp(prefix_workdir("species/{species}/gtpro/snv_counts.tsv.bz2")),
    input:
        script="workflow/scripts/merge_gtpro_counts_for_species.py",  # FIXME: Write this script
        r1=expand(
            prefix_workdir("reads/{mgen}/gtpro/r1.snv_counts.tsv.bz2"),
            mgen=config["all_samples"],
        ),
        r2=expand(
            prefix_workdir("reads/{mgen}/gtpro/r2.snv_counts.tsv.bz2"),
            mgen=config["all_samples"],
        ),
    params:
        species=lambda w: w.species,
        args=expand(
            "{mgen}="
            + prefix_workdir("reads/{mgen}/gtpro/r1.snv_counts.tsv.bz2")
            + ":"
            + prefix_workdir("reads/{mgen}/gtpro/r2.snv_counts.tsv.bz2"),
            mgen=config["all_samples"],
        ),
    threads: 6
    shell:
        """
        {input.script} {params.species} {output} {params.args}
        """


# StrainFacts
## Load
rule load_metagenotype_from_merged_gtpro:
    output:
        prefix_workdir("species/{species}/sfacts/snv_counts.mgtp.nc"),
    input:
        prefix_workdir("species/{species}/gtpro/snv_counts.tsv.bz2"),
    shell:
        """
        sfacts load --gtpro-metagenotype {input} {output}
        """


## Filter
rule filter_metagenotype:
    output:
        temp(
            prefix_workdir(
                "species/{species}/sfacts/{stem}.filt-poly{poly}-cvrg{cvrg}.mgtp.nc"
            )
        ),
    wildcard_constraints:
        poly=integer_wc,
        cvrg=integer_wc,
    input:
        prefix_workdir("species/{species}/sfacts/{stem}.mgtp.nc"),
    params:
        poly=lambda w: float(w.poly) / 100,
        cvrg=lambda w: float(w.cvrg) / 100,
    resources:
        mem_mb=12_000,
    shell:
        """
        sfacts filter_mgen \
                --min-minor-allele-freq {params.poly} \
                --min-horizontal-cvrg {params.cvrg} \
                {input} {output}
        """


## Subsample
rule subset_metagenotype:
    output:
        temp(
            prefix_workdir(
                "species/{species}/sfacts/{stem}.ss-g{num_positions}-block{block_number}-seed{seed}.mgtp.nc"
            )
        ),
    input:
        prefix_workdir("species/{species}/sfacts/{stem}.mgtp.nc"),
    params:
        seed=lambda w: int(w.seed),
        num_positions=lambda w: int(w.num_positions),
        block_number=lambda w: int(w.block_number),
    shell:
        """
        sfacts sample_mgen \
                --random-seed {params.seed} \
                --num-positions {params.num_positions} \
                --block-number {params.block_number} \
                {input} \
                {output}
        """


## Fit
rule fit_sfacts:
    output:
        fit=prefix_workdir(
            "species/{species}/sfacts/{stem}.sfacts-s{strain_exponent}-seed{seed}.sfacts-fit.nc"
        ),
        hist=prefix_workdir(
            "species/{species}/sfacts/{stem}.sfacts-s{strain_exponent}-seed{seed}.loss_history"
        ),
    input:
        mgen=prefix_workdir("species/{species}/sfacts/{stem}.mgtp.nc"),
        strategy=config["sfacts_strategy_path"],
    params:
        strain_exponent=lambda w: float(w.strain_exponent) / 100,
        seed=lambda w: int(w.seed),
    resources:
        mem_mb=5_000,
    shell:
        """
        sfacts fit \
                @{input.strategy} \
                --verbose \
                --random-seed {params.seed} \
                --strain-sample-exponent {params.strain_exponent} \
                --history-outpath {output.hist} \
                -- {input.mgen} {output.fit}
        """


## Export strain details
rule export_sfacts_comm:
    output:
        "{stem}.strain_composition.tsv",
    input:
        "{stem}.sfacts-fit.nc",
    shell:
        """
        sfacts dump --community {output} {input}
        """


## Select strain-pure samples
rule identify_strain_samples:
    output:
        prefix_workdir("species/{species}/spgc/strain_pure_samples.tsv"),
    input:
        prefix_workdir(
            f"species/{{species}}/sfacts/snv_counts.{config['sfacts_infix']}.strain_composition.tsv"
        ),
    params:
        frac_thresh=0.95,
    shell:
        """
        awk -v OFS='\t' -v thresh={params.frac_thresh} 'NR > 1 && $3 > thresh {{print $1,$2}}' {input} > {output}
        """


# StrainPGC
## Prepare Inputs
rule write_species_genes_list:
    output:
        prefix_workdir("species/{species}/spgc/species_genes.list"),
    input:
        config["species_gene_path"],
    shell:
        """
        awk '$2=="{wildcards.species}" {{print $1}}' < {input} > {output}
        """


## Run
rule run_spgc:
    output:
        prefix_workdir("species/{species}/spgc/spgc.results.nc"),
    input:
        pangenome_profile=prefix_workdir(
            "species/{species}/midas/pangenome_profile.depth.tsv.bz2"
        ),
        strain_pure_samples=prefix_workdir(
            "species/{species}/spgc/strain_pure_samples.tsv"
        ),
        species_genes=prefix_workdir("species/{species}/spgc/species_genes.list"),
    params:
        trim_frac_species_genes=0.15,
        species_free_thresh=1e-4,
        depth_ratio_thresh=0.2,
        corr_thresh=0.4,
    shell:
        """
        spgc run \
             --trim-frac-species-genes {params.trim_frac_species_genes} \
             --species-free-thresh {params.species_free_thresh} \
             --depth-ratio-thresh {params.depth_ratio_thresh} \
             --correlation-thresh {params.corr_thresh} \
             {input.pangenome_profile} \
             {input.species_genes} \
             {input.strain_pure_samples} \
             {output}
        """


## Write Gene Content
rule dump_spgc_gene_content:
    output:
        prefix_workdir("species/{species}/spgc.gene.tsv"),
    input:
        spgc=prefix_workdir("species/{species}/spgc/spgc.results.nc"),
    shell:
        "spgc dump_genes {input} {output}"


## Compile metadata
rule dump_spgc_strain_stats:
    output:
        prefix_workdir("species/{species}/spgc.strain.tsv"),
    input:
        spgc=prefix_workdir("species/{species}/spgc/spgc.results.nc"),
    params:
        ambig_thresh=0.1,
    shell:
        "spgc dump_strains {input.spgc} {output}"


# Summarize Workflow
rule summarize_workflow:
    output:
        dot="filegraph.dot",
        pdf="filegraph.pdf",
    input:
        "workflow/Snakefile",
        "config.yaml",
    shell:
        """
        snakemake --filegraph --configfile config.yaml -n all | sed '1,2d' | tee {output.dot} | dot -Tpdf > {output.pdf}
        """
